# Chat Service com LiteLLM, FastAPI e CLI

Este projeto √© um servi√ßo de chat desenvolvido em **Python**.  
O objetivo √© criar um servi√ßo que possa alternar entre diferentes provedores de modelos de IA (como **Groq** e **Fireworks**) de forma transparente, usando a biblioteca **LiteLLM**.  
O servi√ßo √© exposto via uma **API HTTP (FastAPI)** e uma **interface de linha de comando (CLI)**.

---

## üìÇ Estrutura de Diret√≥rios

O projeto segue a seguinte estrutura, com separa√ß√£o de responsabilidades:

```
services/chat_service/
‚îú‚îÄ‚îÄ app/              # C√≥digo da aplica√ß√£o (core, api, cli)
‚îú‚îÄ‚îÄ config/           # Arquivos de configura√ß√£o (models.json, etc.)
‚îú‚îÄ‚îÄ prompts/          # Prompts de valida√ß√£o em arquivos de texto
‚îú‚îÄ‚îÄ runs/             # Logs e telemetria em tempo de execu√ß√£o
‚îú‚îÄ‚îÄ .env.example      # Modelo para vari√°veis de ambiente
‚îú‚îÄ‚îÄ README.md         # Documenta√ß√£o do servi√ßo
```

> Obs.: Lembre-se de adicionar `__init__.py` nas pastas `chat_service/` e `app/` para que sejam reconhecidas como pacotes Python.

---

## üì¶ Depend√™ncias M√≠nimas

Instale as bibliotecas necess√°rias com:

```bash
pip install litellm fastapi uvicorn python-dotenv
```

---

## ‚öôÔ∏è Passo a Passo para Setup e Execu√ß√£o

### 1. Configura√ß√£o do Ambiente

1. Crie um arquivo **.env** em `services/chat_service/` com suas chaves de API:

   ```env
   GROQ_API_KEY=sua_chave_groq
   FIREWORKS_API_KEY=sua_chave_fireworks
   ```

2. Crie o arquivo **models.json** em `config/` com os aliases:

   ```json
   {
     "aliases": {
       "chat/llama-small": {
         "provider": "groq",
         "model": "llama-3.1-8b-instant"
       },
       "chat/qwen-small": {
         "provider": "fireworks",
         "model": "qwen-2-7b-instruct"
       }
     }
   }
   ```

---

### 2. Rodar a API

Na raiz do projeto (`/repo`), execute:

```bash
uvicorn services.chat_service.app.api.main:app --reload
```

O servidor iniciar√° na **porta 8000**.  
Deixe este terminal rodando.

---

### 3. Testar os Endpoints

Abra outro terminal e rode os testes abaixo:

#### üîπ Health Check

```bash
curl http://127.0.0.1:8000/healthz
```

**Resposta esperada:**

```json
{"status":"ok"}
```

---

#### üîπ Listar Modelos

```bash
curl http://127.0.0.1:8000/models
```

**Resposta esperada:**  
Lista dos aliases definidos no `models.json`.

---

#### üîπ Chat

```bash
curl -X POST http://127.0.0.1:8000/chat -H "Content-Type: application/json" -d '{
  "model": "chat/llama-small",
  "messages": [
    {
      "role": "user",
      "content": "Explique, em ate 120 palavras, o que e consistencia eventual e cite 1 caso de uso comum."
    }
  ],
  "params": {
    "temperature": 0.2,
    "top_p": 0.9,
    "max_tokens": 512,
    "seed": 42,
    "stream": false,
    "json_mode": false
  }
}'
```

**Resposta esperada:**  
Um JSON contendo:
- Texto de sa√≠da
- Provedor e modelo usados
- M√©tricas de uso
- Lat√™ncia

---

---

## üñ•Ô∏è Usar a CLI

A **CLI** permite interagir com o servi√ßo diretamente do terminal.

Na raiz do projeto (`/repo`), use o comando `python -m` para executar a CLI.

### üîπ Listar Modelos

```bash
python -m services.chat_service.app.cli.cli models
```

**Resposta esperada:**  
A lista de aliases do seu `models.json`.

---

### üîπ Chat

```bash
python -m services.chat_service.app.cli.cli chat --model chat/llama-small --user "Explique o que e consistencia eventual."
```

**Resposta esperada:**  
Um JSON com:
- A resposta do modelo
- M√©tricas
- Uso de tokens

---
