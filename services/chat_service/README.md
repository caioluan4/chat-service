# Chat Service com LiteLLM, FastAPI e CLI

Este projeto √© um servi√ßo de chat desenvolvido em **Python**.  
O objetivo √© criar um servi√ßo que possa alternar entre diferentes provedores de modelos de IA (como **Groq** e **Fireworks**) de forma transparente, usando a biblioteca **LiteLLM**.  
O servi√ßo √© exposto via uma **API HTTP (FastAPI)** e uma **interface de linha de comando (CLI)**.

---

## üìÇ Estrutura de Diret√≥rios

O projeto segue a seguinte estrutura, com separa√ß√£o de responsabilidades:

```
services/chat_service/
‚îú‚îÄ‚îÄ app/              # C√≥digo da aplica√ß√£o (core, api, cli)
‚îú‚îÄ‚îÄ config/           # Arquivos de configura√ß√£o (models.json, etc.)
‚îú‚îÄ‚îÄ prompts/          # Prompts de valida√ß√£o em arquivos de texto
‚îú‚îÄ‚îÄ runs/             # Logs e telemetria em tempo de execu√ß√£o
‚îú‚îÄ‚îÄ .env.example      # Modelo para vari√°veis de ambiente
‚îú‚îÄ‚îÄ README.md         # Documenta√ß√£o do servi√ßo
```

> Obs.: Lembre-se de adicionar `__init__.py` nas pastas `chat_service/` e `app/` para que sejam reconhecidas como pacotes Python.

---

## üì¶ Depend√™ncias M√≠nimas

Instale as bibliotecas necess√°rias com:

```bash
pip install litellm fastapi uvicorn python-dotenv
```

---

## ‚öôÔ∏è Passo a Passo para Setup e Execu√ß√£o

### 1. Configura√ß√£o do Ambiente

1. Crie um arquivo **.env** em `services/chat_service/` com suas chaves de API:

   ```env
   GROQ_API_KEY=sua_chave_groq
   FIREWORKS_AI_API_KEY=sua_chave_fireworks
   ```

2. Crie o arquivo **models.json** em `config/` com os aliases:

   ```json
   {
     "aliases": {
       "chat/llama-small": {
         "provider": "groq",
         "model": "llama-3.1-8b-instant"
       },
       "chat/qwen-small": {
         "provider": "fireworks_ai",
         "model": "accounts/fireworks/models/qwen2p5-vl-32b-instruct"
       }
     }
   }
   ```

---

### 2. Rodar a API

Na raiz do projeto (`/repo`), execute:

```bash
uvicorn services.chat_service.app.api.main:app --reload
```

O servidor iniciar√° na **porta 8000**.  
Deixe este terminal rodando.

---

### 3. Testar os Endpoints

Abra outro terminal e rode os testes abaixo:

#### üîπ Health Check

```bash
curl http://127.0.0.1:8000/healthz
```

**Resposta esperada:**

```json
{"status":"ok"}
```

---

#### üîπ Listar Modelos

```bash
curl http://127.0.0.1:8000/models
```

**Resposta esperada:**  
Lista dos aliases definidos no `models.json`.

---

#### üîπ Chat

```bash
curl -X POST http://127.0.0.1:8000/chat -H "Content-Type: application/json" -d '{
  "model": "chat/llama-small",
  "messages": [
    {
      "role": "user",
      "content": "Explique, em ate 120 palavras, o que e consistencia eventual e cite 1 caso de uso comum."
    }
  ],
  "params": {
    "temperature": 0.2,
    "top_p": 0.9,
    "max_tokens": 512,
    "seed": 42,
    "stream": false,
    "json_mode": false
  }
}'
```

**Resposta esperada:**  
Um JSON contendo:
- Texto de sa√≠da
- Provedor e modelo usados
- M√©tricas de uso
- Lat√™ncia



---

## üñ•Ô∏è Usar a CLI

A **CLI** permite interagir com o servi√ßo diretamente do terminal.

Na raiz do projeto (`/repo`), use o comando `python -m` para executar a CLI.

### üîπ Listar Modelos

```bash
python -m services.chat_service.app.cli.cli models
```

**Resposta esperada:**  
A lista de aliases do seu `models.json`.

---

### üîπ Chat

```bash
python -m services.chat_service.app.cli.cli chat --model chat/llama-small --user "Explique o que e consistencia eventual."
```

**Resposta esperada:**  
Um JSON com:
- A resposta do modelo
- M√©tricas
- Uso de tokens

---



## ‚ö° Pol√≠tica Ass√≠ncrona (Async)

O servi√ßo de chat foi projetado para ser responsivo e n√£o bloqueante. Para isso, seguimos a seguinte pol√≠tica:

* **L√≥gica do Core**: A fun√ß√£o `chat` em `app/core/chat.py` √© **s√≠ncrona**. Ela lida com a l√≥gica de chamada √† API do LiteLLM de forma simples e direta.
* **API FastAPI**: O endpoint `POST /chat` √© **ass√≠ncrono** (`async`). Ele usa o utilit√°rio `asyncio.to_thread` para executar a fun√ß√£o s√≠ncrona do core em um thread separado.

Essa abordagem garante que a API n√£o seja bloqueada por chamadas de longa dura√ß√£o ao modelo de IA, permitindo que ela processe outras requisi√ß√µes em paralelo e mantendo a alta performance do servi√ßo.



## ‚úÖ Valida√ß√£o de Startup

O servi√ßo executa uma s√©rie de verifica√ß√µes autom√°ticas na inicializa√ß√£o para garantir que o ambiente est√° configurado corretamente. A valida√ß√£o inclui:

* Checagem do arquivo `models.json`.
* Verifica√ß√£o das chaves de API necess√°rias no `.env`.
* Um ping leve em cada provedor configurado para validar a conex√£o e a autentica√ß√£o.

Se alguma dessas valida√ß√µes falhar, o servi√ßo n√£o iniciar√° e exibir√° uma mensagem de erro clara no terminal.

---

## üìä M√©tricas e Telemetria

O servi√ßo gera logs e m√©tricas de telemetria em tempo de execu√ß√£o na pasta `runs/`.

* **`manifest.json`**: Este arquivo de metadados √© gerado por execu√ß√£o (`run_id`). Ele registra informa√ß√µes sobre o ambiente, como o hash do Git (`git_sha`), a vers√£o do Python e as depend√™ncias.
* **`interactions.jsonl`**: Este √© o arquivo principal de logs, onde cada linha √© um JSON contendo os dados de uma intera√ß√£o com o modelo de chat. Ele inclui m√©tricas como `latency_ms` e `usage.total_tokens` para cada requisi√ß√£o.

Voc√™ pode habilitar o log completo da conversa (mensagens de entrada e sa√≠da) definindo a vari√°vel `LOG_MESSAGES` no seu arquivo `.env`:

```env
LOG_MESSAGES=true